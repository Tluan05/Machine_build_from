import numpy as np
import matplotlib.pyplot as plt

# ===============================
# 1. T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p
# ===============================
X = np.array([
    [0.2, 0.7],
    [0.3, 0.3],
    [0.8, 0.5],
    [0.5, 0.1]
])
y = np.array([1, 0, 1, 0])

m, n = X.shape
X = np.c_[np.ones((m, 1)), X]  # th√™m c·ªôt bias
w = np.zeros(n + 1)            # kh·ªüi t·∫°o weight = 0

# ===============================
# 2. H√†m sigmoid, predict v√† loss
# ===============================
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def predict(X, w):
    return sigmoid(np.dot(X, w))

def loss(y, y_pred):
    eps = 1e-15
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return - np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))

# ===============================
# 3. Gradient Descent v·ªõi h·ªôi t·ª•
# ===============================
learning_rate = 0.5
tolerance = 1e-6
max_epochs = 100000

previous_loss = float('inf')
loss_history = []        # üìä L∆∞u loss qua m·ªói v√≤ng
weight_history = []      # üìà L∆∞u c√°c tr·ªçng s·ªë ƒë·ªÉ v·∫Ω qu·ªπ ƒë·∫°o
saved_y_pred = []

for epoch in range(max_epochs):
    y_pred = predict(X, w)
    saved_y_pred.append(y_pred.copy())
    current_loss = loss(y, y_pred)

    # L∆∞u l·ªãch s·ª≠
    loss_history.append(current_loss)
    weight_history.append(w.copy())

    # Gradient
    gradient = np.dot(X.T, (y_pred - y)) / m

    # C·∫≠p nh·∫≠t weight
    w -= learning_rate * gradient

    # Ki·ªÉm tra ƒëi·ªÅu ki·ªán h·ªôi t·ª•
    loss_diff = abs(previous_loss - current_loss)
    grad_norm = np.linalg.norm(gradient)

    if loss_diff < tolerance or grad_norm < tolerance:
        print(f"‚úÖ H·ªôi t·ª• sau {epoch+1} v√≤ng l·∫∑p.")
        break

    previous_loss = current_loss

print("Tr·ªçng s·ªë cu·ªëi c√πng:", w)
print("Loss cu·ªëi c√πng:", current_loss)


plt.figure(figsize=(8,5))
plt.plot(saved_y_pred[0], 'o', label='V√≤ng 1')
plt.plot(saved_y_pred[len(saved_y_pred)//2], 'x', label=f'V√≤ng {len(saved_y_pred)//2}')
plt.plot(saved_y_pred[-1], '.', label=f'V√≤ng {len(saved_y_pred)}')
plt.xlabel('M·∫´u d·ªØ li·ªáu')
plt.ylabel('X√°c su·∫•t d·ª± ƒëo√°n')
plt.title('Qu√° tr√¨nh d·ª± ƒëo√°n qua c√°c v√≤ng l·∫∑p')
plt.legend()
plt.grid(True)
plt.show()

# # ===============================
# # 4. Tr·ª±c quan h√≥a Loss theo epoch
# # ===============================
# plt.figure(figsize=(8,5))
# plt.plot(loss_history, label='Loss')
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.title('Qu√° tr√¨nh h·ªôi t·ª• c·ªßa Loss')
# plt.legend()
# plt.grid(True)
# plt.show()

# # ===============================
# # 5. Tr·ª±c quan h√≥a qu·ªπ ƒë·∫°o tr·ªçng s·ªë (3D)
# # ===============================
# from mpl_toolkits.mplot3d import Axes3D

# weight_history = np.array(weight_history)
# fig = plt.figure(figsize=(8,6))
# ax = fig.add_subplot(111, projection='3d')

# ax.plot(weight_history[:,0], weight_history[:,1], weight_history[:,2], marker='o')
# ax.set_xlabel('Bias (w0)')
# ax.set_ylabel('w1')
# ax.set_zlabel('w2')
# ax.set_title('Qu·ªπ ƒë·∫°o c·∫≠p nh·∫≠t tr·ªçng s·ªë (Weight Trajectory)')
# plt.show()
