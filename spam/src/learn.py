# import numpy as np
# import matplotlib.pyplot as plt

# # ===============================
# # 1. T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p
# # ===============================
# X = np.array([
#     [0.2, 0.7],
#     [0.3, 0.3],
#     [0.8, 0.5],
#     [0.5, 0.1]
# ])
# y = np.array([1, 0, 1, 0])

# m, n = X.shape
# X = np.c_[np.ones((m, 1)), X]  # th√™m c·ªôt bias
# w = np.zeros(n + 1)            # kh·ªüi t·∫°o weight = 0
# print(X)

import numpy as np
import matplotlib.pyplot as plt

# ===============================
# 1. T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p
# ===============================
X = np.array([
    [0.2, 0.7],
    [0.3, 0.3],
    [0.8, 0.5],
    [0.5, 0.1]
])
y = np.array([1, 0, 1, 0])

m, n = X.shape
X = np.c_[np.ones((m, 1)), X]  # th√™m c·ªôt bias
w = np.zeros(n + 1)            # kh·ªüi t·∫°o weight = 0

# ===============================
# 2. H√†m sigmoid, predict v√† loss
# ===============================
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def predict(X, w):
    return(np.dot(X, w))

def loss(y, y_pred):
    eps = 1e-15
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return - np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))

# ===============================
# 3. Gradient Descent v·ªõi h·ªôi t·ª•
# ===============================
learning_rate = 0.5
tolerance = 1e-6
max_epochs = 100000

previous_loss = float('inf')
loss_history = []        # üìä L∆∞u loss qua m·ªói v√≤ng
weight_history = []      # üìà L∆∞u c√°c tr·ªçng s·ªë ƒë·ªÉ v·∫Ω qu·ªπ ƒë·∫°o
saved_y_pred = []

for epoch in range(max_epochs):
    y_pred = predict(X, w)
    if (epoch <10):
        print(y_pred)
    saved_y_pred.append(y_pred.copy())
    current_loss = loss(y, y_pred)

    # L∆∞u l·ªãch s·ª≠
    loss_history.append(current_loss)
    weight_history.append(w.copy())

    # Gradient
    gradient = np.dot(X.T, (y_pred - y)) / m

    # C·∫≠p nh·∫≠t weight
    w -= learning_rate * gradient

    # Ki·ªÉm tra ƒëi·ªÅu ki·ªán h·ªôi t·ª•
    loss_diff = abs(previous_loss - current_loss)
    grad_norm = np.linalg.norm(gradient)

    if loss_diff < tolerance or grad_norm < tolerance:
        print(f"‚úÖ H·ªôi t·ª• sau {epoch+1} v√≤ng l·∫∑p.")
        break

    previous_loss = current_loss
print(saved_y_pred[-5:])
print("Tr·ªçng s·ªë cu·ªëi c√πng:", w)
print("Loss cu·ªëi c√πng:", current_loss)


# plt.figure(figsize=(8,5))
# plt.plot(saved_y_pred[0], 'o', label='V√≤ng 1')
# plt.plot(saved_y_pred[len(saved_y_pred)//2], 'x', label=f'V√≤ng {len(saved_y_pred)//2}')
# plt.plot(saved_y_pred[-1], '.', label=f'V√≤ng {len(saved_y_pred)}')
# plt.xlabel('M·∫´u d·ªØ li·ªáu')
# plt.ylabel('X√°c su·∫•t d·ª± ƒëo√°n')
# plt.title('Qu√° tr√¨nh d·ª± ƒëo√°n qua c√°c v√≤ng l·∫∑p')
# plt.legend()
# plt.grid(True)
# plt.show()
